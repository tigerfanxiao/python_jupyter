# Windows Terminal
## Switch Tab
`ctrl + tab`

# Markdown in VS code
* `ctrl + shift + v` open an preview window 
* `ctrl + k, v` open an side by side preview window

# Ubuntu
## Clear Screen
```shell
clear
```
`ctrl + L`
## update
```shell
sudo apt update
sudo apt upgrade -y
```
If you cannot run upgrade, it is because auto upgrade is running in the background. You could test with `sudo apt update` to see the quantity of upgradeable package is decreasing.

## check ps
```shell
ps aux | grep -i apt
```
### kill ps
```shell
sudo kill <process_id>
```

## lsb_release
```shell
sudo apt install lsb-core
lsb_release -a
```
## Save command output to variable
```shell
my_var = 'pwd'
// or 
my_var = $(pwd)
// print variable
echo $my_var
```

## Create soft link
create soft link at /usr/bin/bar to /opt/foo
```shell 
ln -s /object/folder /softlink/file
```
if the object folder is renamed, the soft link will break

## Config ssh

```shell
// check if ssh public key already exists
ls -l ~/.ssh/id_*.pub

// generate ssh key
ssh-keygen -t rsa -b 4096 -C "your_email@domain.com"
```
edit ssh config
[refer](https://www.digitalocean.com/community/questions/error-permission-denied-publickey-when-i-try-to-ssh)
```shell
sudo nano /etc/ssh/sshd_config
// PermitRootLogin prohibit-password to 
PermitRootLogin yes 

// PasswordAuthentication no to 
PasswordAuthentication yes
```
then, restart ssh service:
```shell
// if you cannot restart ssh service
sudo ssh-keygen -A

// restart ssh service
sudo service ssh restart
// or
/etc/init.d/ssh restart
// or
sudo systemctl restart ssh

// add private key to ssh-agent
sudo ssh-add ~/.ssh/id_rsa
```
test ssh
```shell
ssh -T git@github.com
Hi tigerfanxiao! You've successfully authenticated, but GitHub does not provide shell access.
```


# Python

## Pandas

```python
import pandas as pd
```

### Create Data Frame

```python
# define the data as a list
data = [
    ("Dexter","Johnsons","dog","shiba inu","red sesame",1.5,35,"m",False,"both",True),
    ("Alfred","Johnsons","cat","mix","tuxedo",4,12,"m",True,"indoor",True),
    ("Petra","Smith","cat","ragdoll","calico",6,None,"f",False,"both",True),
    ("Ava","Smith","dog","mix","blk/wht",12,32,"f",True,"both",False),
    ("Schroder","Brown","cat","mix","orange",13,15,"m",False,"indoor",True),
    ("Blackbeard","Brown","bird","parrot","multi",5,3,"f",False,"indoor",),
]

# define the labels
labels = ["name","owner","type","breed","color","age","weight","gender","health issues","indoor/outboor","vaccinated"]

# create dataframe
vet_records = pd.DataFrame.from_records(data, columns=labels)

```

### view dataframe

```python
# show data
vet_records.head() # show first 5 records
vet_records.tail() # show last 5 records
vet_records # show first 5 records

# show data type of each column
vet_records.dtypes

# show statistic information on the columns that operation can be performed on 
vet_records.describe()
vet_records.describe(include='all')

```

### Edit Data

```python
# change the value of one cell
vet_records.at[0,'weight'] = 30.8

# add one new column
vet_records.assign(new_col_name(vet_records['age']/vet_records['weight']))
```

### Slicing and Filtering

```python
# show one column
vet_records.type # type column
vet_records['type'] # type column

# groupby is used for whole dataframe
vet_records.groupby('type')
# for one column use value count
vet_records.type.value_counts()

# filter with all column
vet_records[vet_records.type == 'dog']
# filter with one columns
vet_records.type[vet_records.type=='dog']

```

### loc & iloc

- `loc` allows you to use column names to slice data
- `iloc` requires the use of index numbers. Example: `.iloc[row, column]`. Remember: python indexes starting at 0.

```python
# show the first 3 rows, column name and column owner
vet_records.loc[0:3, ['name', 'owner']]

# show the first column
vet_records.iloc[:, 0]

# get 3rd and 4th row , with field color and age
vet_records.iloc[[2,3], [4,5]]

# in
vet_records[vet_records.name.isin(['Dexter', 'Blackbeard'])]
# not 
vet_records[~vet_records.name.isin(['Dexter', 'Blackbeard'])]

```

### mask

```python
# filter the table with mask
mask = vet_records.gender=="m"
vet_records[mask]

```

### None and NaN

`.isna` will create a boolean dataframe True where the value is NaN or None. It is advisable to deal with NaN and None values before doing ny calculations. A NaN and None cell are ignored during calculations.

```python
# show a table and see which value is na
vet_records.isna()

# fill all the na cell with 0
vet_records.fillna(0)

# fillna with dictionay
values = {'age': 12, 'vaccinated': False}
vet_records.fillna(value=values) # return a new dataframe instance

```



## Matplotlib

install matplotlib

```shell
conda install matplotlib
```



```python
import matplotlib.pyplot as plt
import numpy as np
%matplotlib inline

# set x to be 0 to 4pi in .1 increment
x = np.arange(0, 4*np.pi, 0.1)

y = np.sin(x)
z = np.sin(x)

# plot them
plt.plot(x, y, x, z)
plt.show()
```

```javascript
var s = "JavaScript syntax highlighting";
alert(s);
```
```python
s = "Python syntax highlighting"
print s
```



# Jupyter

package is needed to later turn our notebooks to PDF
```shell
sudo apt install -y texlive-xetex
```
Download anaconda
```shell
cd /tmp
curl -O https://repo.anaconda.com/archive/Anaconda3-2019.03-Linux-x86_64.sh
```
check md5
```shell
sha256sum Anaconda3-2019.03-Linux-x86_64.sh
```
> 45c851b7497cc14d5ca060064394569f724b67d9b5f98a926ed49b834a6bb73a Anaconda3-2019.03-Linux-x86_64.sh

install anaconda
```shell
bash Anaconda3-2019.03-Linux-x86_64.sh
```
During install, you could choose auto config env path.
This will update you `~/.bashrc` file so that you can use conda on the command.

We need to restart the shell to include the changes made:
```shell
source ~/.bashrc
```
You will see a `(base)` prepended to your command line. That indicates that the base conda virtual environment has been activated. This will happen everytime we start the terminal. We don't want that behavior; to turn it off:
```shell
conda config --set auto_activate_base false
```
Now let's update the conda base code:
```shell
conda update -n base -c defaults conda
```
## Create virtual env
```shell
// create env
conda create -n python_data_course python=3
// list all available env
conda env list

// activate env
conda activate envname
// deactivate current
conda deactive 
```
### Install jupyter
```shell
conda install jupyter psutil
conda install matplotlib
conda install pandas numpy
```
### start and stop jupyter server
```shell
jupyter --version
jupyter notebook list
// stop jupyter notebook by the port
jupyter notebook stop 8086
```
### run jupyter on remove server
run this python script
```python
import os
import psutil
from subprocess import Popen
from time import sleep

no_token = True

while no_token:
    # kill any jupyter notebook processes
    for proc in psutil.process_iter():
        if proc.name() == 'jupyter-noteboo':
            proc.kill()

    # delete old nohup.out if exists
    if os.path.exists("nohup.out"):
        os.remove("nohup.out")

    # start jupyter notebook server on port 8086
    Popen(
        ['nohup', 'jupyter', 'notebook', '--no-browser', '--port=8086'],
        stdout=open('nohup.out', 'w'),
        preexec_fn=os.setpgrp)

    # wait for above process to finish
    sleep(3)

    # write jupyter server token to screen
    token_file = open("nohup.out")

    for line in token_file:
        if line.find("token=") > 0:
            print(line[line.find("token=")+6: ])
            no_token = False
            break

    token_file.close()

```
## Open Jupyter notebook on remote sever
In a local terminal window, enter the following:
```shell
ssh -N -L localhost:8087:localhost:8086 cloud_user@<the public IP address of the Playground server>
```
`-N` indicates there will be no remote commands. `-L` maps the local port to the remote port.

We've selected port 8086, but the actual port number is not important as long as we know what it is and it does not conflict with other running services.

It will ask for a password. This is the password we use to log in to the Playground remote server.

Leave this terminal open. It will appear nothing has happened, but it must remain open while we use the Jupyter Notebook server in this session.

In a browser, enter the following address:

http://localhost:8087

This will open a Jupyter Notebook site that asks for the token copied from the remote server

After you input the token, you need to the jupyter server

# PosgreSQL

## install

```shell
sudo apt update
sudo apt install postgresql postgresql-contrib
```

### Gain Access to PSQL Command Line

- `psql` is the interactive terminal for working with PostgreSQL

At the command line:

- `sudo -u postgres psql`

You are not logged in as the "postgres" superuser.

#### Create Database

- `CREATE DATABASE cloud_user;`

#### Create User

- `CREATE USER cloud_user WITH ENCRYPTED PASSWORD 'cloud_user';`

#### Grant Access to Database by User

- `GRANT ALL PRIVILEGES ON DATABASE cloud_user TO cloud_user;`

You now have a database you can access the named cloud_user as the user `cloud_user`.

#### Leave PSQL

- `\q`

### Install Postgresql Driver to Your Virtual Environment

- `conda activate python_data_course`
- `conda install psycopg2`

### Server Operations Using Python's Psycopg2

```python
import pandas as pd
import psycopg2

CONNECT_DB = "host=localhost port=5432 dbname=cloud_user user=cloud_user password=cloud_user"
```

### Create Table

```python
create_table_query = '''CREATE TABLE tips (
    ID SERIAL PRIMARY KEY,
    weekday varchar (10),
    meal_type varchar (10),
    wait_staff varchar (10),
    party_size smallint,
    meal_total float4,
    tip float4
); '''

try:
    # Make connection to db
    cxn = psycopg2.connect(CONNECT_DB)

    # Create a cursor to db
    cur = cxn.cursor()

    # Send sql query to request
    cur.execute(create_table_query)
    records = cxn.commit()

except (Exception, psycopg2.Error) as error :
    print ("Error while connecting to PostgreSQL", error)

finally:
    #closing database connection.
    if(cxn):
        cur.close()
        cxn.close()
        print("PostgreSQL connection is closed")

print(f'Records:\n {records}')
```

### Add the Data to Table

```python
try:
    # Make connection to db
    cxn = psycopg2.connect(CONNECT_DB)

    # Create a cursor to db
    cur = cxn.cursor()

    with open('./tips.csv', 'r') as f:
        # skip first row, header row
        next(f)
        cur.copy_from(f, 'tips', sep=",")
        cxn.commit()

except (Exception, psycopg2.Error) as error :
    print ("Error while connecting to PostgreSQL", error)

finally:
    #closing database connection.
    if(cxn):
        cur.close()
        cxn.close()
        print("PostgreSQL connection is closed")
        print("tips table populated")
```

### Selecting Data From a Server

Use `.fetchall()` with LIMIT or TOP (#)

- LIMIT works for most databases but does not work with SQL Server
- TOP (#) is used in place of LIMIT on SQL Server

```python
def db_server_fetch(sql_query):
    try:
        # Make connection to db
        cxn = psycopg2.connect(CONNECT_DB)

        # Create a cursor to db
        cur = cxn.cursor()

        # Send sql query to request
        cur.execute(sql_query)
        records = cur.fetchall()

    except (Exception, psycopg2.Error) as error :
        print ("Error while connecting to PostgreSQL", error)

    finally:
        #closing database connection.
        if(cxn):
            cur.close()
            cxn.close()
            print("PostgreSQL connection is closed")
        return records
```

The test table is populated by `Select`ing the first five rows.

```python
select_query = '''SELECT * FROM tips LIMIT 5;'''

records = db_server_fetch(select_query)
print(records)
def db_server_change(sql_query):
    try:
        # Make connection to db
        cxn = psycopg2.connect(CONNECT_DB)

        # Create a cursor to db
        cur = cxn.cursor()

        # Send sql query to request
        cur.execute(sql_query)
        records = cxn.commit()

    except (Exception, psycopg2.Error) as error :
        print ("Error while connecting to PostgreSQL", error)

    finally:
        #closing database connection.
        if(cxn):
            cur.close()
            cxn.close()
            print("PostgreSQL connection is closed")
        return records
```

Add a new record with the following data: On Saturday, new wait staff Alfred had one person at Breakfast for 10.76 and received a 0.50 tip.

```python
add_data = '''INSERT INTO tips
    (id, weekday, meal_type, wait_staff, party_size, meal_total, tip)
    VALUES
    (504, 'Saturday', 'Breakfast', 'Alfred', 1, 10.76, 0.50);'''

db_server_change(add_data)
```

#### Make a `SELECT` Request to Get New Records

```python
select_query = '''SELECT * FROM tips WHERE wait_staff='Alfred';'''

records = db_server_fetch(select_query)
print(records)
```

### Accessing a SQL Database With Pandas

pandas.read_sql( ) - loads data from database pandas.to_sql( ) - write data to database

**CAUTION:** Please don't write to a database unless you know what you are doing and are authorized. If you are not, your permission should allow read-only.

```python
def pandas_db_server_fetch(sql_query):
    try:
        # Make connection to db
        cxn = psycopg2.connect(CONNECT_DB)

        # Send sql query to request and create dataframe
        df = pd.read_sql(sql_query, cxn)

    except (Exception, psycopg2.Error) as error :
        print ("Error while connecting to PostgreSQL", error)

    finally:
        #closing database connection.
        if(cxn):
            cxn.close()
            print("PostgreSQL connection is closed")
        return df
select_query = '''SELECT * FROM tips WHERE wait_staff='Alfred';'''

alfred_df = pandas_db_server_fetch(select_query)
alfred_df.head()
tips_df = pandas_db_server_fetch('''SELECT * FROM tips;''')
tips_df.head()
```

# MongoDB

### Installing MongoDB on the Playground Server

Information on installation is taken from [MongoDB docs](https://docs.mongodb.com/manual/tutorial/install-mongodb-on-ubuntu/). Information on remote access can be found at [How to connect to your remote MongoDB server](https://ianlondon.github.io/blog/mongodb-auth/).

Import the GPG key from MongoDB. 

```wget -qO - https://www.mongodb.org/static/pgp/server-4.2.asc | sudo apt-key add -```

Create a list file for MongoDB. 

```echo "deb [ arch=amd64,arm64 ] https://repo.mongodb.org/apt/ubuntu bionic/mongodb-org/4.2 multiverse" | sudo tee /etc/apt/sources.list.d/mongodb-org-4.2.list```

Reload the package database. `sudo apt update`

Install MongoDB. `sudo apt install -y mongodb-org`

Start the MongoDB service. `sudo systemctl start mongod`

This starts the MongoDB service as needed. To have the MongoDB service start every time we start the server: `sudo systemctl enable mongod`

We only start the service when necessary, but this is an option.

### Create User, Database, and Grant Access

#### Create User

At the terminal: `bash mongo`

Now at the mongo terminal: 

```
mongo use cloud_user
db.createUser({
    user: 'cloud_user',
    pwd: 'cloud_user',
    roles: [{ role: 'readWrite', db:'cloud_user'}]
})

```

Leave the mongo shell with `CTRL-c`.

#### Create Collection from File

*A special thanks to MongoDB for the zips dataset.*

Import collection: `mongoimport -v --db=cloud_user --file=/home/cloud_user/python_data_course/data/zips.json`

### Install MongoDB Driver to Our Virtual Environment

```shell
conda activate python_data_course
conda install pymongo
```

### Start and Connect to the Jupyter Notebook Server as Usual

### Using Python to Connect to the Server

#### Imports and DB Connection

```python
from pymongo import MongoClient
import pandas as pd

client = MongoClient("mongodb://cloud_user:cloud_user@localhost:27017/cloud_user")
df = pd.DataFrame.from_records(client.cloud_user.zips.find())
df.head()
oz_data = {"_id": 99990, "city": "EMERALD", "loc": [-510.9, -600.89], "pop": 564372, "state": "Munchkin Land"}
client.cloud_user.zips.insert_one(oz_data)
df1 = pd.DataFrame.from_records(client.cloud_user.zips.find())
df1.tail()
df.count()
df1.count()
client.close()
```

# CSV

pandas has a function named `read_csv()` that will read a `csv` file directly into a DataFrame. This method takes the following arguments:

- `filepath`
- `sep=','`: This has a default of ',', but we encourage its use so when we encounter a file delimited with something else, we know how to set it.
- `header=0`: This tells it to treat the first line as the column names. Refer to documentation for other options.

There are other arguments to skip blank lines, set encoding, and provide column names. Please refer to the documentation for more information.

#### Read `tips.csv` into a DataFrame

```python
import pandas as pd

tips_df = pd.read_csv('./tips.csv', sep=',', header=0)
tips_df.head()
```

#### Add the `tip_percent` Column and Calculation to the DataFrame

```python
percent_tip = pd.Series(tip_df['tip']/tips_df['meal_total'],name='tip_percent')
tips_df = pd.concat([tips_df, percent_tip], axis=1)
tips_df.head()
```

### Writing CSV Files with pandas

Documentation for [pandas.to_csv()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_csv.html).

pandas has a function named `to_csv()` that will write a DataFrame to a `csv` file. This method takes the following arguments:

- filepath
- `sep=','`: This has a default of ',', but we encourage its use.
- `header=True`: This tells it to treat the first line as the column names.
- `index=False`: Prevents index values being written to the file.

There are other arguments. Please refer to the documentation for more information.

#### Write `tips_df` to `tips_percent.csv`

```python
tips_df.to_csv('tips_percent.csv', sep=',', header=True, index=False)
```

#### Check the File was Written Correctly by Reading into a New `df`

```python
tips_df_from_file = pd.read_csv('./tips_percent.csv', sep=',', header=0)
tips_df_from_file.head()
```

# Excel

### pandas and Excel

- pandas can read and write to Excel
- pandas uses openpyxl, install `conda install openpyxl xlrd`
- [pandas.to_excel() documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_excel.html)
- [pandas.read_excel() documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_excel.html)

#### Read tips.xlsx as a dataframe

```python
import pandas as pd

tips_df = pd.read_excel('tips.xlsx', index_col=0)  

tips_df.head()
```

#### Create a separate df for each meal type

```python
breakfast_df = tips_df[tips_df.meal_type=='Breakfast']
lunch_df = tips_df[tips_df.meal_type=='Lunch']
dinner_df = tips_df[tips_df.meal_type=='Dinner']
```

#### Use pd.to_excel() to create an Excel workbook with the breakfast data

```python
breakfast_df.to_excel("breakfast_tips.xlsx")  
```

#### Test the file is created and has data

```python
breakfast_tips_df = pd.read_excel('breakfast_tips.xlsx', index_col=0)  

breakfast_tips_df.head()
```

#### Write Excel file meal_type_tips.xlsx with a worksheet for each meal-type and one for the original data

**NOTE:** *It is advised to keep a copy of the original data. So, I suggest you always save to a new file.*

To write to separate worksheets, we will use ExcelWriter with `.to_excel()`.

```python
with pd.ExcelWriter('meal_type_tips.xlsx') as writer:  
    breakfast_df.to_excel(writer, sheet_name='breakfast')
    lunch_df.to_excel(writer, sheet_name='lunch')
    dinner_df.to_excel(writer, sheet_name='dinner')
    tips_df.to_excel(writer, sheet_name='tips_orig')
```

To read all sheets in as an ordered_dict:

```python
meal_type_tips_df = pd.read_excel('meal_type_tips.xlsx', sheet_name=None)  

meal_type_tips_df.keys()
meal_type_tips_df['breakfast']
```

To read each sheet into a dataframe separately, use argument `sheet_name`.

```python
breakfast_tips_df = pd.read_excel('meal_type_tips.xlsx', sheet_name='breakfast')  

breakfast_tips_df.head()
```

# LaTex

LaTex is a document preparation system that is designed to ease the creation of technical and scientific documents. It is really good at displaying mathematical formulas and is built-in to Jupyter Notebooks. Depending on your use case, it may be something you need.

In this lesson, we are going to cover the basic use of LaTex to add mathematical formulas to your report.

You can easily find many examples of how to use LaTex on the web. Just search for `latex matrices` or whatever subject you need.

[LaTex Documentation](https://www.latex-project.org/help/documentation/)

#### Inline LaTex Equations

LaTex formulas are written in a Markdown cell. Inline formulas are those that are within a sentence.

The Pythagorean theorem is a^2 + b^2 = c^2.

While the above statement is factual, it doesn't look nice for a report or publication,

The Pythagorean theorem is $a^2 + b^2 = c^2$. Created with `$a^2 + b^2 = c^2$`.

This looks much better and is generated by placing `$` at the beginning and the end of the equation. Inline formulas can be designated with `$`.

#### Latex Equations Not Inline

LaTex formulas that are not inline are denoted with a `$$` at the start and end of the formula. Be default the formulas will be centered.

Greek characters are denoted by a `\` and the Greek letter. For example \beta results in $\beta$.

To create a matrix use `\begin{matrix}` and end with `\end{matix}`.

```
$$
\begin{bmatrix}
\alpha& \beta^{*}\\
\gamma^{*}& \delta
\end{bmatrix}    
$$
```

becomes: $$ \begin{bmatrix} \alpha& \beta^{*}\ \gamma^{*}& \delta \end{bmatrix}
$$
Here is one last example demonstrating a summation and a fraction.

```
$$
\sum_{i=1}^{n}i=\frac{n(n+1)}{2}
$$
```

Becomes:
$$
\sum_{i=1}^{n}i=\frac{n(n+1)}{2}
$$
学习方法

如果你没有办法用简单的语言, 描述你所学会的知识, 那么你就没有真正学会它

